# ðŸš€ Serve API Auto-Generation & Running Guide

Tá»± Ä‘á»™ng sinh code API tá»« serve config vÃ  cháº¡y luÃ´n - khÃ´ng cáº§n lÃ m thá»§ cÃ´ng!

## âš¡ Quick Start (CÃ¡ch nhanh nháº¥t)

### CÃ¡ch 1: Bash Script (Recommended)

```bash
# Make script executable (chá»‰ cáº§n cháº¡y 1 láº§n)
chmod +x serve_api.sh

# Cháº¡y FastAPI
./serve_api.sh mlproject/configs/generated/standard_train_serve.yaml

# Cháº¡y Ray Serve trÃªn port 9000
./serve_api.sh -f ray -p 9000 mlproject/configs/generated/standard_train_serve.yaml
```

### CÃ¡ch 2: Python Script

```bash
# FastAPI (default)
python serve_api.py --serve-config mlproject/configs/generated/standard_train_serve.yaml

# Ray Serve
python serve_api.py \
    --serve-config mlproject/configs/generated/standard_train_serve.yaml \
    --framework ray \
    --port 9000
```

### CÃ¡ch 3: Python Module

```bash
python -m mlproject.serve.run_generated_api \
    --serve-config mlproject/configs/generated/standard_train_serve.yaml \
    --framework fastapi \
    --port 8000
```

---

## ðŸ“‹ Táº¥t cáº£ cÃ¡c Options

```bash
python serve_api.py \
    --serve-config <path_to_serve.yaml>     # Required: Serve config
    --train-config <path_to_train.yaml>     # Optional: Auto-inferred náº¿u khÃ´ng cÃ³
    --framework <fastapi|ray>               # Optional: Default fastapi
    --host <host>                           # Optional: Default 0.0.0.0
    --port <port>                           # Optional: Default 8000
```

---

## ðŸŽ¯ Examples

### Example 1: Standard Single Model

```bash
# FastAPI
./serve_api.sh mlproject/configs/generated/standard_train_serve.yaml

# Ray Serve
./serve_api.sh -f ray mlproject/configs/generated/standard_train_serve.yaml
```

### Example 2: Conditional Branch (Multi-Model)

```bash
# FastAPI
./serve_api.sh mlproject/configs/generated/conditional_branch_serve.yaml

# Ray Serve vá»›i custom port
./serve_api.sh -f ray -p 9000 mlproject/configs/generated/conditional_branch_serve.yaml
```

### Example 3: KMeans + XGBoost Pipeline

```bash
./serve_api.sh mlproject/configs/generated/kmeans_then_xgboost_serve.yaml
```

### Example 4: Custom Host & Port

```bash
python serve_api.py \
    --serve-config mlproject/configs/generated/standard_train_serve.yaml \
    --host 127.0.0.1 \
    --port 5000
```

---

## ðŸ”§ Quy trÃ¬nh tá»± Ä‘á»™ng

Khi cháº¡y script, nÃ³ sáº½ tá»± Ä‘á»™ng:

1. **Generate API code** tá»« serve.yaml
2. **Configure** host vÃ  port
3. **Run** API server ngay láº­p tá»©c

```
[1/3] Generating API code...
âœ“ Generated: mlproject/serve/generated/standard_train_serve_fastapi.py

[2/3] Configuring server settings...
âœ“ Configured: 0.0.0.0:8000

[3/3] Starting FASTAPI server...

============================================================
ðŸš€ API is starting at: http://0.0.0.0:8000
ðŸ“– API docs: http://0.0.0.0:8000/docs
â¤ï¸  Health check: http://0.0.0.0:8000/health
============================================================

ðŸ’¡ Press Ctrl+C to stop the server
```

---

## ðŸ“Š API Endpoints

Sau khi server cháº¡y:

### Health Check
```bash
curl http://localhost:8000/health
```

Response:
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

### Prediction
```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "data": {
      "feature1": [1.0, 2.0, 3.0],
      "feature2": [4.0, 5.0, 6.0]
    }
  }'
```

Response:
```json
{
  "predictions": [0.123, 0.456, 0.789]
}
```

### Interactive Docs (FastAPI only)
Má»Ÿ browser: `http://localhost:8000/docs`

---

## ðŸŽ¨ Framework Comparison

### FastAPI
âœ… **Pros:**
- Lightweight, nhanh
- Auto-generated docs (Swagger UI)
- Dá»… debug
- Synchronous (Ä‘Æ¡n giáº£n)

âŒ **Cons:**
- Single process (khÃ´ng scale tá»± Ä‘á»™ng)
- Pháº£i dÃ¹ng load balancer Ä‘á»ƒ scale

**Best for:** Development, small deployments, single-model serving

### Ray Serve
âœ… **Pros:**
- Distributed, scale tá»± Ä‘á»™ng
- Multi-replica (load balancing built-in)
- Async processing
- Dashboard Ä‘á»ƒ monitor
- Production-ready

âŒ **Cons:**
- Phá»©c táº¡p hÆ¡n
- Tá»‘n resource hÆ¡n
- Setup phá»©c táº¡p hÆ¡n

**Best for:** Production, high-traffic, multi-model serving

---

## ðŸ› ï¸ Troubleshooting

### Port already in use
```bash
# DÃ¹ng port khÃ¡c
./serve_api.sh -p 9000 mlproject/configs/generated/standard_train_serve.yaml
```

### Module not found
```bash
# Äáº£m báº£o Ä‘ang á»Ÿ thÆ° má»¥c root
cd /home/user/ml_pipeline_template

# Hoáº·c set PYTHONPATH
export PYTHONPATH=$(pwd):$PYTHONPATH
```

### Model not loading
- Check MLflow tracking URI: `echo $MLFLOW_TRACKING_URI`
- Verify model exists: `mlflow models list`
- Check alias: Máº·c Ä‘á»‹nh lÃ  "production"

### Import errors
```bash
# Install dependencies
pip install fastapi uvicorn ray[serve]
```

---

## ðŸ“ Generated Files Location

Táº¥t cáº£ generated files Ä‘Æ°á»£c lÆ°u trong:
```
mlproject/serve/generated/
â”œâ”€â”€ standard_train_serve_fastapi.py
â”œâ”€â”€ standard_train_serve_ray.py
â”œâ”€â”€ conditional_branch_serve_fastapi.py
â”œâ”€â”€ conditional_branch_serve_ray.py
â””â”€â”€ ...
```

Báº¡n cÃ³ thá»ƒ:
- âœ… Chá»‰nh sá»­a Ä‘á»ƒ customize
- âœ… Check vÃ o git Ä‘á»ƒ track changes
- âœ… Deploy trá»±c tiáº¿p lÃªn production
- âœ… Regenerate báº¥t cá»© lÃºc nÃ o

---

## ðŸ”„ Workflow Complete

### Development Flow
```bash
# 1. Train model
python -m mlproject.src.pipeline.dag_run train ...

# 2. Generate serve config
python -m mlproject.src.pipeline.dag_run generate ...

# 3. Run API (Tá»° Äá»˜NG SINH CODE!)
./serve_api.sh mlproject/configs/generated/standard_train_serve.yaml

# 4. Test
curl http://localhost:8000/health
```

### One-Liner Development
```bash
# Generate vÃ  run ngay
python -m mlproject.src.pipeline.dag_run generate \
    mlproject/configs/pipelines/standard_train.yaml \
    --config-type serve \
    --output-dir mlproject/configs/generated \
  && ./serve_api.sh mlproject/configs/generated/standard_train_serve.yaml
```

---

## ðŸ’¡ Pro Tips

### 1. Cháº¡y nhiá»u APIs cÃ¹ng lÃºc (different ports)
```bash
# Terminal 1: Model A
./serve_api.sh -p 8000 mlproject/configs/generated/standard_train_serve.yaml

# Terminal 2: Model B
./serve_api.sh -p 8001 mlproject/configs/generated/conditional_branch_serve.yaml
```

### 2. Background running
```bash
# FastAPI vá»›i nohup
nohup ./serve_api.sh mlproject/configs/generated/standard_train_serve.yaml > api.log 2>&1 &

# Check log
tail -f api.log
```

### 3. Docker deployment
```dockerfile
FROM python:3.11

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt

# Run API on container start
CMD ["python", "serve_api.py", \
     "--serve-config", "mlproject/configs/generated/standard_train_serve.yaml", \
     "--host", "0.0.0.0", \
     "--port", "8000"]
```

### 4. Kubernetes deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: api
        image: your-ml-api:latest
        command:
          - python
          - serve_api.py
          - --serve-config
          - mlproject/configs/generated/standard_train_serve.yaml
        ports:
        - containerPort: 8000
```

---

## ðŸ“š Related Documentation

- **API Generation**: `mlproject/serve/generated/README.md`
- **Config Generation**: `mlproject/src/utils/generator/README.md` (if exists)
- **Example Scripts**: `examples/generate_serve_apis.py`

---

## ðŸŽ“ Summary

**CÃ¡ch dÃ¹ng Ä‘Æ¡n giáº£n nháº¥t:**
```bash
./serve_api.sh <serve_config.yaml>
```

**That's it!** ðŸŽ‰

Script sáº½:
1. âœ… Tá»± Ä‘á»™ng generate code
2. âœ… Tá»± Ä‘á»™ng configure
3. âœ… Tá»± Ä‘á»™ng run server
4. âœ… Hiá»ƒn thá»‹ URLs Ä‘á»ƒ test

**KhÃ´ng cáº§n lÃ m gÃ¬ thá»§ cÃ´ng ná»¯a!**

"""Feature inference step for eval/serve modes."""

from __future__ import annotations

from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from omegaconf import OmegaConf

from mlproject.src.pipeline.steps.core.base import PipelineStep
from mlproject.src.pipeline.steps.core.constants import DataTypes, ModelTypes
from mlproject.src.pipeline.steps.core.factory import StepFactory
from mlproject.src.pipeline.steps.core.utils import (
    ConfigAccessor,
    SampleAligner,
    WindowBuilder,
)


class FeatureInferenceStep(PipelineStep):
    """Generate features from loaded model in eval/serve mode.

    This step is auto-generated by ConfigGenerator for eval/serve
    pipelines when feature_pipeline is declared in train config.

    It handles:
    1. Loading base features from context
    2. Composing with additional features if specified
    3. Using loaded model to generate features
    4. Storing features in specified output_key

    This eliminates manual inference step creation and ensures
    consistency between train/eval/serve pipelines.
    """

    def __init__(
        self,
        step_id: str,
        cfg: Any,
        enabled: bool = True,
        depends_on: Optional[List[str]] = None,
        source_model_key: str = "model",
        base_features_key: str = "preprocessed_data",
        output_key: str = "features",
        experiment_config: Optional[Any] = None,
        inference_method: str = "predict",
        apply_windowing: bool = True,
        additional_feature_keys: Optional[List[str]] = None,
        feature_align_method: str = "auto",
        **kwargs: Any,
    ) -> None:
        """Initialize feature inference step."""
        super().__init__(
            step_id,
            cfg,
            enabled,
            depends_on,
            additional_feature_keys=additional_feature_keys,
            feature_align_method=feature_align_method,
            **kwargs,
        )
        self.source_model_key = source_model_key
        self.base_features_key = base_features_key
        self.output_key = output_key
        self.inference_method = inference_method
        self.apply_windowing = apply_windowing

        # Merge experiment config to access global settings (e.g. data type, chunks)
        if experiment_config is None:
            self.effective_cfg = cfg
        else:
            self.effective_cfg = OmegaConf.merge(cfg, experiment_config)

    def _get_model_and_input(self, context: Dict[str, Any]) -> tuple[Any, np.ndarray]:
        """Get model and prepared input from context.

        If additional_feature_keys are specified, compose features
        from base + additional sources before returning.
        """
        model = context.get(self.source_model_key)
        if model is None:
            raise KeyError(
                f"Step '{self.step_id}': Model key "
                f"'{self.source_model_key}' not found in context"
            )

        base_features = context.get(self.base_features_key)
        if base_features is None:
            raise KeyError(
                f"Step '{self.step_id}': Base features key "
                f"'{self.base_features_key}' not found in context"
            )

        # Compose with additional features if specified
        if self.additional_feature_keys:
            composed_features = self._compose_features_for_inference(
                context, base_features
            )
            return model, self._prepare_input(composed_features)

        return model, self._prepare_input(base_features)

    def _compose_features_for_inference(
        self,
        context: Dict[str, Any],
        base_features: Any,
    ) -> pd.DataFrame:
        """Compose base features with additional sources for inference.

        Parameters
        ----------
        context : Dict[str, Any]
            Pipeline context.
        base_features : Any
            Base feature data.

        Returns
        -------
        pd.DataFrame
            Composed features.
        """
        # Convert base to DataFrame
        if isinstance(base_features, np.ndarray):
            base_df = pd.DataFrame(base_features)
        elif isinstance(base_features, pd.DataFrame):
            base_df = base_features.copy()
        else:
            base_df = pd.DataFrame(base_features)

        composed = base_df.copy()
        n_samples = len(composed)

        print(f"[{self.step_id}] Composing features for inference:")
        print(f"  Base: {composed.shape}")

        for key in self.additional_feature_keys:
            additional = context.get(key)
            if additional is None:
                print(f"  Warning: Feature key '{key}' not found, skipping")
                continue

            # Convert to DataFrame
            if isinstance(additional, np.ndarray):
                if additional.ndim == 1:
                    additional = pd.DataFrame({f"{key}_0": additional})
                else:
                    cols = [f"{key}_{i}" for i in range(additional.shape[1])]
                    additional = pd.DataFrame(additional, columns=cols)
            elif not isinstance(additional, pd.DataFrame):
                additional = pd.DataFrame(additional)

            # Align samples
            additional = self._align_samples_for_inference(additional, n_samples)

            # Force index alignment
            if len(additional) == len(composed):
                additional.index = composed.index

            # Concatenate
            composed = pd.concat([composed, additional], axis=1)
            print(f"  + {key}: {additional.shape} -> Total: {composed.shape}")

        return composed

    def _align_samples_for_inference(
        self,
        df: pd.DataFrame,
        target_samples: int,
    ) -> pd.DataFrame:
        """Align DataFrame to target number of samples using SampleAligner."""
        # Force update for pylint check
        # Create base with target length
        base_arr = np.zeros((target_samples, 1))

        # Align using SampleAligner
        _, aligned = SampleAligner.align_samples(base_arr, df, method="auto")

        # Convert back to DataFrame
        result = pd.DataFrame(aligned, columns=df.columns)
        return result

    def _get_model_expected_features(self, model: Any) -> Optional[int]:
        """Get the number of features the model expects.

        Checks common attributes used by sklearn and other models.
        Returns None if cannot be determined.
        """
        # sklearn models
        if hasattr(model, "n_features_in_"):
            return model.n_features_in_

        # Some wrappers store the inner model
        if hasattr(model, "model"):
            inner = model.model
            if hasattr(inner, "n_features_in_"):
                return inner.n_features_in_

        # XGBoost specific
        if hasattr(model, "n_features_"):
            return model.n_features_

        return None

    def _should_apply_windowing_runtime(
        self, model: Any, x: np.ndarray, data_type: str
    ) -> bool:
        """Determine at runtime if windowing is needed.

        This provides a safety check by comparing the model's expected
        feature count with the input feature count.
        """
        if not DataTypes.is_timeseries(data_type):
            return False

        # Get model's expected feature count
        expected_features = self._get_model_expected_features(model)
        if expected_features is None:
            # Can't detect, fall back to config flag
            return self.apply_windowing

        input_features = x.shape[1] if x.ndim > 1 else 1

        # If model expects more features than input has, windowing is needed
        if expected_features > input_features:
            config_accessor = ConfigAccessor(self.effective_cfg)
            window_config = config_accessor.get_window_config()
            input_chunk = window_config["input_chunk_length"]
            expected_windowed = input_features * input_chunk

            if expected_features == expected_windowed:
                print(
                    f"  Auto-detected: model expects {expected_features} features "
                    f"(windowed), input has {input_features}"
                )
                return True

        # If model expects same features as input, no windowing needed
        if expected_features == input_features:
            return False

        # Fall back to config flag
        return self.apply_windowing

    def _run_windowed_inference(
        self, model: Any, x: np.ndarray, method_name: str
    ) -> np.ndarray:
        """Run inference with windowing."""
        config_accessor = ConfigAccessor(self.effective_cfg)
        window_config = config_accessor.get_window_config()
        input_chunk = window_config["input_chunk_length"]
        output_chunk = window_config["output_chunk_length"]

        # Use WindowBuilder for windowing
        x_windows = WindowBuilder.create_windows(x, input_chunk, output_chunk)
        print(f"  Windowed shape: {x_windows.shape}")

        model_type_value = getattr(model, "model_type", None)
        is_dl_model = (
            ModelTypes.is_sequence_model(model_type_value)
            if model_type_value
            else False
        )

        if not is_dl_model and x_windows.ndim == 3:
            x_windows = x_windows.reshape(x_windows.shape[0], -1)

        inference_func = getattr(model, method_name)
        features = inference_func(x_windows)

        if features.ndim == 3:
            features = features.reshape(features.shape[0], -1)

        return features

    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute feature generation."""
        self.validate_dependencies(context)

        model, x = self._get_model_and_input(context)

        config_accessor = ConfigAccessor(self.effective_cfg)
        data_type = config_accessor.get_data_type()

        method_name = self.inference_method
        if not hasattr(model, method_name):
            if method_name == "predict" and hasattr(model, "transform"):
                print(
                    f"[{self.step_id}] Model missing 'predict', "
                    "falling back to 'transform'"
                )
                method_name = "transform"
            else:
                raise AttributeError(
                    f"Step '{self.step_id}': Model '{self.source_model_key}' "
                    f"has no method '{method_name}'"
                )

        print(
            f"[{self.step_id}] Generating features from "
            f"'{self.source_model_key}' using '{method_name}'"
        )
        print(f"  Input shape: {x.shape}")
        print(
            f"  data_type: {data_type}, config apply_windowing: {self.apply_windowing}"
        )

        # Use runtime detection to determine if windowing is needed
        # This checks the model's expected feature count vs input feature count
        # to handle cases where config flag might be incorrect
        should_window = self._should_apply_windowing_runtime(model, x, data_type)
        print(f"  runtime apply_windowing: {should_window}")

        if DataTypes.is_timeseries(data_type) and should_window:
            features = self._run_windowed_inference(model, x, method_name)
        else:
            inference_func = getattr(model, method_name)
            features = inference_func(x)

        # Store in context
        context[self.output_key] = features
        print(
            f"[{self.step_id}] Stored features in '{self.output_key}' "
            f"(shape: {features.shape})"
        )

        return context

    def _prepare_input(self, features: Any) -> np.ndarray:
        """Prepare input for model prediction.

        Parameters
        ----------
        features : Any
            Raw features (DataFrame, ndarray, etc).

        Returns
        -------
        np.ndarray
            Prepared input array.
        """

        if isinstance(features, pd.DataFrame):
            return features.values.astype(np.float64)

        if isinstance(features, np.ndarray):
            return features.astype(np.float64)

        if isinstance(features, (list, tuple)):
            return np.array(features, dtype=np.float64)

        raise TypeError(f"Unsupported feature type: {type(features).__name__}")


StepFactory.register("feature_inference", FeatureInferenceStep)

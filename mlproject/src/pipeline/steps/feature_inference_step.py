"""Feature inference step for eval/serve modes."""

from __future__ import annotations

from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from omegaconf import OmegaConf

from mlproject.src.pipeline.steps.base import BasePipelineStep
from mlproject.src.pipeline.steps.factory_step import StepFactory


class FeatureInferenceStep(BasePipelineStep):
    """Generate features from loaded model in eval/serve mode.

    This step is auto-generated by ConfigGenerator for eval/serve
    pipelines when feature_pipeline is declared in train config.

    It handles:
    1. Loading base features from context
    2. Using loaded model to generate features
    3. Storing features in specified output_key

    This eliminates manual inference step creation and ensures
    consistency between train/eval/serve pipelines.
    """

    def __init__(
        self,
        step_id: str,
        cfg: Any,
        enabled: bool = True,
        depends_on: Optional[List[str]] = None,
        source_model_key: str = "model",
        base_features_key: str = "preprocessed_data",
        output_key: str = "features",
        experiment_config: Optional[Any] = None,
        **kwargs: Any,
    ) -> None:
        """Initialize feature inference step."""
        super().__init__(step_id, cfg, enabled, depends_on, **kwargs)
        self.source_model_key = source_model_key
        self.base_features_key = base_features_key
        self.output_key = output_key

        # Merge experiment config to access global settings (e.g. data type, chunks)
        if experiment_config is None:
            self.effective_cfg = cfg
        else:
            self.effective_cfg = OmegaConf.merge(cfg, experiment_config)

    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute feature generation."""
        self.validate_dependencies(context)

        # Get model
        model = context.get(self.source_model_key)
        if model is None:
            raise KeyError(
                f"Step '{self.step_id}': Model key "
                f"'{self.source_model_key}' not found in context"
            )

        # Get base features
        base_features = context.get(self.base_features_key)
        if base_features is None:
            raise KeyError(
                f"Step '{self.step_id}': Base features key "
                f"'{self.base_features_key}' not found in context"
            )

        # Check data type
        data_cfg = self.effective_cfg.get("data", {})
        data_type = str(data_cfg.get("type", "tabular")).lower()

        # Prepare input
        x = self._prepare_input(base_features)

        print(f"[{self.step_id}] Generating features from '{self.source_model_key}'")
        print(f"  Input shape: {x.shape}")

        if data_type == "timeseries":
            # Apply windowing
            hyperparams = self.effective_cfg.get("experiment", {}).get(
                "hyperparams", {}
            )
            input_chunk = hyperparams.get("input_chunk_length", 1)
            output_chunk = hyperparams.get("output_chunk_length", 1)

            x_windows = self._create_windows(x, input_chunk, output_chunk)
            print(f"  Windowed shape: {x_windows.shape}")

            # Predict
            features = model.predict(x_windows)

            # Pad output to align with original length
            n = len(x)
            expected_len = len(features)

            # Alignment assumption: last input point aligns with prediction time?
            # Or standard windowing: start_align = input_chunk.
            # We align end of window. Windows start at index `input_chunk` (if stride=1 logic).
            start_align = input_chunk

            # Correction: _create_windows iterates range(input_chunk, n - output_chunk + 1)
            # So first window ends at index `input_chunk`.
            # If we want alignment such that feature[t] corresponds to input[t],
            # then the prediction for window ending at t is available at t.

            if len(features) < n:
                # Initialize with NaNs
                full_preds = np.full((n, 1), np.nan)
                if features.ndim > 1:
                    full_preds = np.full((n, features.shape[1]), np.nan)

                # Careful with alignment.
                # Window 0 uses x[0:input_chunk]. Corresponds to time step `input_chunk-1`.
                # Let's align it to `input_chunk-1`?
                # FrameworkModelStep used `start_align = input_chunk - 1`.

                start_align = input_chunk - 1
                end_align = start_align + expected_len

                if end_align <= n:
                    full_preds[start_align:end_align] = features.reshape(
                        expected_len, -1
                    )
                    features = full_preds
                    print(f"  Padded features from {expected_len} to {n}")

        else:
            # Tabular
            features = model.predict(x)

        print(f"  Output shape: {features.shape}")

        # Store in context
        context[self.output_key] = features
        print(f"[{self.step_id}] Stored features in '{self.output_key}'")

        return context

    def _create_windows(
        self, x: np.ndarray, input_chunk: int, output_chunk: int
    ) -> np.ndarray:
        """Create sliding windows from input array."""
        n = len(x)
        windows = []
        stride = 1

        # Logic matching BaseDataModule._create_windows
        for end_idx in range(input_chunk, n - output_chunk + 1, stride):
            start_idx = end_idx - input_chunk
            # Flatten window
            win = x[start_idx:end_idx].reshape(-1)
            windows.append(win)

        if not windows:
            return np.empty((0, input_chunk * x.shape[1]))

        return np.stack(windows)

    def _prepare_input(self, features: Any) -> np.ndarray:
        """Prepare input for model prediction.

        Parameters
        ----------
        features : Any
            Raw features (DataFrame, ndarray, etc).

        Returns
        -------
        np.ndarray
            Prepared input array.
        """

        if isinstance(features, pd.DataFrame):
            return features.values.astype(np.float32)

        if isinstance(features, np.ndarray):
            return features.astype(np.float32)

        if isinstance(features, (list, tuple)):
            return np.array(features, dtype=np.float32)

        raise TypeError(f"Unsupported feature type: {type(features).__name__}")


StepFactory.register("feature_inference", FeatureInferenceStep)

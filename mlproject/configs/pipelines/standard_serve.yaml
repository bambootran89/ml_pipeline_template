pipeline:
  name: "standard_serve"
  steps:
    # Step 1: Preprocessing (load from MLflow)
    - id: "preprocess"
      type: "preprocessor"
      enabled: true
      is_train: false  # CRITICAL: Load from MLflow!
      alias: "latest"  # Override via --alias in CLI

    # Step 2: Load model from MLflow Registry
    - id: "load_model"
      type: "model_loader"
      enabled: true
      train_step_id: "train_model"
      depends_on: ["preprocess"]
      alias: "latest"  # Override vá»›i --alias (latest/production/staging)

    # Step 3: Run inference
    - id: "inference"
      type: "inference"
      enabled: true
      depends_on: ["preprocess", "load_model"]
      model_step_id: "load_model"
      # Output: inference_predictions in context
